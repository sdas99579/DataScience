# -*- coding: utf-8 -*-
"""M200722CA_SOURAV_Kmeans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1puP5a8PbAY3wkemT3Bz2cZCbznimeWOp
"""

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.cluster import KMeans,DBSCAN
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import  linkage
from scipy.cluster.hierarchy import dendrogram
from scipy.cluster.hierarchy import cut_tree

import warnings

warnings.filterwarnings("ignore",category=DeprecationWarning)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib
# %matplotlib inline
from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name = fn,length = len(uploaded[fn])))

uploaded

df=pd.read_csv('iris.data')

df.head()

#count if each species
df.groupby("classlabel\t").size()

#checking the metadata information
df.info()

#checking how data is spread
df.describe()

#visulalizing data
sns.pairplot(df,hue='classlabel\t')

#correlation
plt.figure(figsize=(6,6))
corrmat = df.corr()
sns.heatmap(corrmat,annot=True,square=True,cmap='YlGnBu')
plt.show()

#checking for outliers
cols = set(df.columns)-{'classlabel\t'}
count = 1
plt.subplots(figsize=(6,6))
for i in cols:
  plt.subplot(2,2,count)
  sns.boxplot(df[i])
  count +=1
plt.show()

#data modelling
X=df.drop(['classlabel\t'],axis=1)
Y=df['classlabel\t']
#checking shape of data
print(X.shape)
#checking shape of target variable
print(Y.shape)

#calculating the hopkins statistic
from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
import numpy as np
from math import isnan

def hopkins(X):
    d=X.shape[1]
    n=len(X) #rows
    m=int(0.1 * n)
    nbrs= NearestNeighbors(n_neighbors=1).fit(X.values)
    rand_X = sample(range(0,n,1),m)

    ujd = []
    wjd = []

    for j in range(0,m):
      u_dist, _= nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1,-1), 2,return_distance=True)
      ujd.append(u_dist[0][1])
      w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1,-1),2,return_distance=True)
      wjd.append(w_dist[0][1])


    H = sum(ujd) / (sum(ujd) + sum(wjd))
    if isnan(H):
      print(ujd, wjd)
      H=0
    return H

#hopkins measure
hopkins(X)

#applying KMeans clusturing algo
kmeans = KMeans(n_clusters=5)
y_kmeans = kmeans.fit_predict(X)
print(y_kmeans)

kmeans.cluster_centers_

##apply silhouette analysis
sse_ = []
for k in range(2,10):
  kmeans = KMeans(n_clusters=k).fit(X)
  sse_.append([k,silhouette_score(X,kmeans.labels_)])

plt.plot(pd.DataFrame(sse_)[0],pd.DataFrame(sse_)[1]);

#Lets use the elbow curve method to identify the ideal number of clusters.

Error=[]
for i in range(1, 11):
  kmeans = KMeans(n_clusters=i).fit(X)
  kmeans.fit(X)
  Error.append(kmeans.inertia_)
import matplotlib.pyplot as plt
plt.plot(range(1,11), Error)
plt.title('Elbow Method')
plt.xlabel('No of clusters')
plt.ylabel('Error')
plt.show()

#final model with k=3
kmeans = KMeans(n_clusters=3,max_iter=300)
kmeans.fit(X)

kmeans.labels_

#assig the label 
X_grouped = X.copy()
X_grouped['cluster_id'] = kmeans.labels_
X_grouped.head()

#checking how many data points are assigned to each cluster ids
X_grouped['cluster_id'].value_counts()

#Box plot for cluster id versus sepal length
sns.boxplot(x='cluster_id',y='sepallength', data=X_grouped)

#scatter plat for Sepal length Versus sepal width
sns.scatterplot(x='sepallength',y='sepalwidth', hue='cluster_id', data=X_grouped,palette=['green','blue','red'])

#Hierarchical Clustering

#single linkage
mergings = linkage(X, method="single", metric='euclidean')
dendrogram(mergings)
plt.show()

#above we identify 2 clusters
cluster_labels__single = cut_tree(mergings, n_clusters=2).reshape(-1, )
cluster_labels__single

#assign cluster labels
X_grouped['cluster_labels_single'] = cluster_labels__single
X_grouped.head()

#checking how many data points are assign to each cluster ids
X_grouped['cluster_labels_single'].value_counts()

#complete linkage
mergings = linkage(X, method='complete', metric='euclidean')
dendrogram(mergings)
plt.show()

#from above we identify 3 clusters
cluster_labels_complete = cut_tree(mergings,n_clusters=3).reshape(-1, )
cluster_labels_complete

#assign cluster labels
X_grouped['cluster_labels_complete'] = cluster_labels_complete
X_grouped.head()

#checking how many data points are assigned to each cluster ids
X_grouped['cluster_labels_complete'].value_counts()

#box plot for cluster id versus sepal length
sns.boxplot(x='cluster_labels_complete', y='sepallength', data=X_grouped)

#scater plot for Sepal Length Versus Sepal Width
sns.scatterplot(x='sepallength', y='sepalwidth', hue='cluster_labels_complete', data= X_grouped, palette=['green','blue','red'])

#scater plot for petal length Versus petal Width
sns.scatterplot(x='petallength', y='petalwidth', hue='cluster_labels_complete', data= X_grouped, palette=['green','blue','red'])

# Guassian mixture model

from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=3)
gmm.fit(X)

#predictions from gmm
labels = gmm.predict(X)
plt.scatter(X.iloc[:,0],X.iloc[:,1],c=labels)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")

#assign cluster labels
X_grouped['cluster_labels_gmm'] = labels
#checking how many data points are assigned to each cluster ids
X_grouped['cluster_labels_gmm'].value_counts()

#Prediction the data
#kmeans prediction
y_pred_kmeans = kmeans.predict(X)
#Hierarchical clustering
y_pred_hierarchical = cut_tree(mergings, n_clusters=3).reshape(-1, )
#GMM clustering
y_pred_gmm = gmm.predict(X)

#checking ARI(Adjusted Rand Index)
from sklearn.metrics.cluster import adjusted_rand_score
#k-means performance
print("ARI of KMeans = ",adjusted_rand_score(Y,y_pred_kmeans))

#Hierarchical performance
print("ARI of Hierarchical = ",adjusted_rand_score(Y,y_pred_hierarchical))

#GMM performance
print("ARI of GMM = ",adjusted_rand_score(Y,y_pred_gmm))

